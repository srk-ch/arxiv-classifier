# ğŸ“ arXiv Paper Classifier

An AI-powered web application that automatically classifies research papers into academic categories using machine learning. Built with Flask and CatBoost, achieving 87.56% classification accuracy.

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![Flask](https://img.shields.io/badge/Flask-2.0+-green.svg)
![CatBoost](https://img.shields.io/badge/CatBoost-ML-orange.svg)

## âœ¨ Features

- **AI-Powered Classification**: Uses CatBoost machine learning model with 87.56% accuracy
- **5 Academic Categories**: AI/ML, Physics, Mathematics, Biology & Health, Chemistry & Materials
- **User Authentication**: Secure signup/login with email verification
- **Classification History**: Track all your paper classifications
- **Admin Dashboard**: Comprehensive user management and analytics
- **Confidence Scoring**: View top 5 predictions with confidence percentages
- **Beautiful UI**: Modern, responsive design with gradient effects and animations
- **Email Notifications**: Automated verification emails via Flask-Mail

## ğŸš€ Demo

### Main Classifier Interface
- Input paper title and abstract
- Get instant classification results
- View confidence scores and top predictions
- Access classification history

### Admin Panel
- View all users and their statistics
- Monitor total classifications
- View user-specific classification history
- Delete users (non-admin only)

## ğŸ› ï¸ Tech Stack

### Backend
- **Python 3.8+**
- **Flask** - Web framework
- **SQLAlchemy** - ORM for database management
- **Flask-Mail** - Email verification system
- **CatBoost** - Machine learning model
- **Scikit-learn** - Feature extraction (TF-IDF)
- **Joblib** - Model serialization

### Frontend
- **HTML5/CSS3** - Modern, responsive design
- **JavaScript (Vanilla)** - Dynamic interactions
- **Google Fonts** - Typography (Space Grotesk, Inter)

### Database
- **SQLite** - Lightweight database for development

## ğŸ“‹ Prerequisites

- Python 3.8 or higher
- pip (Python package manager)
- Gmail account (for email verification)

## ğŸ”§ Installation

### Option 1: Use Pre-trained Model (Quick Start)

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/arxiv-classifier.git
cd arxiv-classifier
```

2. **Create virtual environment**
```bash
python -m venv venv

# On Windows
venv\Scripts\activate

# On macOS/Linux
source venv/bin/activate
```

3. **Install dependencies**
```bash
pip install flask flask-sqlalchemy flask-mail catboost scikit-learn joblib numpy scipy werkzeug
```

4. **Download pre-trained model files**

The following files should be in the root directory:
- `catboost_hybrid.cbm` - Trained CatBoost model
- `vectorizer.pkl` - TF-IDF vectorizer
- `keywords.pkl` - Domain-specific keywords

5. **Configure email settings**

Edit `app.py` and update email configuration:
```python
app.config['MAIL_USERNAME'] = 'your-email@gmail.com'
app.config['MAIL_PASSWORD'] = 'your-app-password'
app.config['MAIL_DEFAULT_SENDER'] = 'your-email@gmail.com'
```

**Note**: For Gmail, you need to generate an [App Password](https://support.google.com/accounts/answer/185833).

6. **Initialize database**
```bash
python app.py
```

This will create the SQLite database and default admin account.

### Option 2: Train Your Own Model (Advanced)

Want to improve accuracy or train on your own dataset? Follow these steps:

1. **Open the training notebook**

Access the Google Colab notebook for model training:
```
ğŸ““ [Model Training Notebook](https://colab.research.google.com/drive/1EKN2jFlSww5aK7DFrfGf2l_8A3jGyB0l)
```

2. **Prepare your data**

The notebook uses the arXiv metadata snapshot. You can:
- Use the default arXiv dataset (15,000 papers, 3,000 per category)
- Increase `target_per_class` in the notebook to train on more samples
- Or provide your own dataset in the required format

**Dataset format** (if using custom data):
```python
# JSON format (one paper per line)
{
  "title": "Paper title here",
  "abstract": "Abstract text here",
  "categories": "cs.LG stat.ML"
}
```

3. **Run the training notebook**

The notebook consists of 3 main blocks:

**Block 1 - Setup** (30 seconds)
- Mount Google Drive
- Install dependencies (CatBoost, scikit-learn, etc.)

**Block 2 - Dataset Creation** (2-3 minutes)
- Loads arXiv metadata
- Creates balanced dataset (3,000 papers per category)
- Cleans text (removes LaTeX, mathematical notation)
- Splits into train/validation/test sets
- Current categories:
  - AI & ML: cs.LG, cs.AI, cs.CL, cs.CV, cs.NE, cs.RO, stat.ML
  - Physics: hep-ph, hep-th, astro-ph, gr-qc, quant-ph, nucl-th, nucl-ex
  - Mathematics: math.AG, math.AT, math.CO, math.DG, math.NT, math.PR, math.ST
  - Biology & Health: q-bio.BM, q-bio.GN, q-bio.NC, q-bio.QM, q-bio.SC
  - Chemistry & Materials: cond-mat.mtrl-sci, cond-mat.str-el, etc.

**Block 3 - Model Training** (3-4 minutes)
- Hybrid approach: TF-IDF (15,000 features) + domain-specific keywords
- CatBoost classifier with optimized hyperparameters
- Early stopping to prevent overfitting
- Automatically saves 3 files:
  - `catboost_hybrid.cbm`
  - `vectorizer.pkl`
  - `keywords.pkl`

**Final Cell - Download Files**
- Downloads all 3 trained files to your computer

4. **Customize training (optional)**

**Increase training samples:**
```python
# In Block 2, change this line:
target_per_class = 3000  # Change to 5000, 10000, etc.
```

**Adjust model parameters:**
```python
# In Block 3, modify CatBoost parameters:
cat = CatBoostClassifier(
    iterations=400,           # More iterations = better accuracy (slower)
    learning_rate=0.15,       # Lower = more careful learning
    depth=6,                  # Deeper trees = more complex patterns
    early_stopping_rounds=50  # Patience before stopping
)
```

**Add custom keywords:**
```python
# In Block 2, extend DOMAIN_KEYWORDS dictionary:
DOMAIN_KEYWORDS = {
    'AI_ML': ['neural', 'network', 'deep', 'learning', 'transformer', ...],
    'Physics': ['quantum', 'particle', 'photon', ...],
    # Add more keywords to improve classification
}
```

5. **Download trained files**

After training completes, the last cell automatically downloads:
- `catboost_hybrid.cbm` (trained model)
- `vectorizer.pkl` (TF-IDF vectorizer)
- `keywords.pkl` (domain keywords)

6. **Replace model files**

Copy the downloaded files to your project root directory:
```bash
# Navigate to your project directory
cd arxiv-classifier

# Backup old models (optional but recommended)
mkdir models_backup
cp catboost_hybrid.cbm vectorizer.pkl keywords.pkl models_backup/

# Copy new trained models from Downloads
cp ~/Downloads/catboost_hybrid.cbm .
cp ~/Downloads/vectorizer.pkl .
cp ~/Downloads/keywords.pkl .
```

7. **Test your new model**
```bash
python app.py
```

Your app now uses your custom-trained model! ğŸ‰

**Expected Training Time:**
- Setup: ~30 seconds
- Dataset creation: 2-3 minutes
- Model training: 3-4 minutes
- **Total: ~6-8 minutes** âš¡

## ğŸ® Usage

### Starting the Application

```bash
python app.py
```

The application will be available at `http://127.0.0.1:5000`

### Default Admin Account

- **Username**: `admin`
- **Password**: `admin123`
- **Email**: `admin@example.com`

âš ï¸ **Important**: Change the admin password after first login!

### User Workflow

1. **Sign Up**: Create an account with username, email, and password
2. **Verify Email**: Check your email for verification link
3. **Login**: Access the classifier interface
4. **Classify Papers**: Enter title and abstract for classification
5. **View Results**: See predictions, confidence scores, and history
6. **Track History**: Access all previous classifications via sidebar

### Admin Workflow

1. **Login** as admin
2. **View Dashboard**: See user statistics and system metrics
3. **Manage Users**: View user details and classification history
4. **Delete Users**: Remove non-admin users if needed

## ğŸ“Š Supported Categories

| Category | Icon | Description |
|----------|------|-------------|
| AI & Machine Learning | ğŸ¤– | Artificial intelligence, neural networks, deep learning |
| Physics | âš›ï¸ | Theoretical, experimental, and applied physics |
| Mathematics | ğŸ“ | Pure and applied mathematics, algebra, geometry |
| Biology & Health | ğŸ§¬ | Biological sciences, medical research, health sciences |
| Chemistry & Materials | ğŸ§ª | Chemistry, materials science, related fields |

## ğŸ—ï¸ Project Structure

```
arxiv-classifier/
â”‚
â”œâ”€â”€ app.py                      # Main Flask application
â”œâ”€â”€ catboost_hybrid.cbm         # Trained ML model
â”œâ”€â”€ vectorizer.pkl              # TF-IDF vectorizer
â”œâ”€â”€ keywords.pkl                # Domain keywords
â”œâ”€â”€ arxiv_classifier.db         # SQLite database
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ home.html              # Landing page
â”‚   â”œâ”€â”€ about.html             # About project page
â”‚   â”œâ”€â”€ aboutus.html           # Team information
â”‚   â”œâ”€â”€ signup.html            # User registration
â”‚   â”œâ”€â”€ login.html             # User login
â”‚   â”œâ”€â”€ classifier.html        # Main classifier interface
â”‚   â””â”€â”€ admin.html             # Admin dashboard
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ model_training.ipynb   # Google Colab notebook for training
â”‚
â””â”€â”€ README.md                  # This file
```

## ğŸ§  Model Training

### Training Your Own Model

We provide a **Google Colab notebook** that trains the model in just **6-8 minutes**! ğŸš€

**Access the Training Notebook**: [ğŸ““ Google Colab Training Notebook](https://colab.research.google.com/drive/1EKN2jFlSww5aK7DFrfGf2l_8A3jGyB0l)

### What the Notebook Does

The notebook automatically:

1. **Block 1 - Setup** (~30 seconds)
   - Mounts Google Drive
   - Installs required packages (CatBoost, scikit-learn, tqdm)
   - Imports necessary libraries

2. **Block 2 - Dataset Creation** (2-3 minutes)
   - Loads arXiv metadata snapshot from Google Drive
   - Builds balanced dataset: **15,000 papers** (3,000 per category)
   - Text preprocessing:
     - Removes LaTeX notation and mathematical symbols
     - Cleans and normalizes text
     - Filters papers with <30 words
   - Splits data: 70% train, 15% validation, 15% test

3. **Block 3 - Hybrid Model Training** (3-4 minutes)
   - **TF-IDF Features**: 15,000 features with bigrams
   - **Keyword Features**: Domain-specific keyword matching
   - **CatBoost Classifier**: Fast gradient boosting
   - **Early Stopping**: Prevents overfitting
   - Automatically saves all 3 model files

4. **Final Cell - Download**
   - One-click download of all trained files

### Training Architecture

**Hybrid Feature Engineering:**
```python
TF-IDF Vectorization (15,000 features)
    +
Domain-Specific Keywords (50+ per category)
    =
Enhanced Classification Accuracy
```

**Categories & Keywords:**

| Category | arXiv Categories | Keywords (examples) |
|----------|------------------|---------------------|
| AI & ML | cs.LG, cs.AI, cs.CV, cs.CL, cs.NE, stat.ML | neural, transformer, attention, lstm, cnn |
| Physics | hep-ph, hep-th, astro-ph, gr-qc, quant-ph | quantum, particle, photon, entanglement |
| Mathematics | math.AG, math.AT, math.CO, math.DG, math.NT | theorem, proof, manifold, topology |
| Biology & Health | q-bio.BM, q-bio.GN, q-bio.NC, q-bio.QM | gene, protein, dna, crispr, genome |
| Chemistry & Materials | cond-mat.mtrl-sci, cond-mat.str-el, cond-mat.soft | crystal, lattice, superconductor, synthesis |

### Customization Options

**1. Increase Training Samples**
```python
# Block 2: Change target_per_class
target_per_class = 3000  # Default
# Change to: 5000, 10000, or more for better accuracy
```

**2. Adjust Model Hyperparameters**
```python
# Block 3: Modify CatBoost settings
cat = CatBoostClassifier(
    iterations=400,           # More = better (but slower)
    learning_rate=0.15,       # Lower = more careful learning
    depth=6,                  # Deeper trees = more complexity
    eval_metric='Accuracy',
    early_stopping_rounds=50
)
```

**3. Tune TF-IDF Features**
```python
# Block 3: Adjust vectorizer
vec = TfidfVectorizer(
    ngram_range=(1,2),        # (1,3) for trigrams
    max_features=15000,       # Increase to 20000, 25000
    min_df=2,                 # Minimum document frequency
    max_df=0.95              # Maximum document frequency
)
```

**4. Add Custom Keywords**
```python
# Block 2: Extend DOMAIN_KEYWORDS
DOMAIN_KEYWORDS = {
    'AI_ML': ['neural', 'network', 'deep', 'learning', 'transformer', 
              'attention', 'lstm', 'cnn', 'rnn', 'gradient'],
    # Add more keywords to improve accuracy
}
```

### Training Performance

**Expected Results:**
- Training Time: 20-30 minutes total
- Final Accuracy: ~87-90% on test set
- Model Size: ~50-100 MB (all 3 files combined)

**Output Files:**
1. `catboost_hybrid.cbm` - Trained CatBoost model
2. `vectorizer.pkl` - TF-IDF vectorizer with vocabulary
3. `keywords.pkl` - Domain-specific keyword dictionary

### Requirements for Training

- Google account (for Colab)
- arXiv metadata snapshot in Google Drive (or download during training)
- ~2 GB RAM (provided free by Colab)
- Internet connection

### Data Source

The notebook uses the official [arXiv metadata snapshot](https://www.kaggle.com/datasets/Cornell-University/arxiv):
- 2.4+ million research papers
- Metadata includes: title, abstract, categories, authors
- JSON format (one paper per line)
- Updated regularly by arXiv

**Note**: You can download this dataset from Kaggle or use your own paper collection!

## ğŸ” Security Features

- Password hashing using Werkzeug
- Email verification before account activation
- Session management for authenticated users
- Admin-only routes with authorization decorators
- SQL injection protection via SQLAlchemy ORM
- CSRF protection (implement Flask-WTF for production)

## ğŸš€ Deployment

### Production Recommendations

1. **Change Secret Key**
```python
app.config['SECRET_KEY'] = 'your-secure-random-key-here'
```

2. **Use Production Database**
```python
app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://user:pass@host/db'
```

3. **Set Debug to False**
```python
app.run(debug=False)
```

4. **Use Production WSGI Server**
```bash
pip install gunicorn
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

5. **Enable HTTPS** with SSL certificates

6. **Implement Rate Limiting** for API endpoints

## ğŸ‘¥ Team

### Research Team
- **Seetharama Kartheek** - [LinkedIn](https://www.linkedin.com/in/srkch/)
- **Shaik Sameed** - [LinkedIn](https://www.linkedin.com/in/sk-sameed-0909s)

### Design Team
- **K. Sri Akshaya** - [LinkedIn](https://www.linkedin.com/in/sri-akshaya-kodali-619279323/)
- **P V Sai Tejaswi** - [LinkedIn](https://www.linkedin.com/in/tejaswi-putluri-815228291/)

### Engineering Team
- **Karani Sumana Sree** - [LinkedIn](https://www.linkedin.com/in/sumana-sree-karani-13a888300)

## ğŸ“§ Contact

For questions or feedback, reach out to:
- Email: seetharamakartheekchallapalli@gmail.com

## ğŸ“ License

This project is licensed under the CC BY-ND - see the LICENSE file for details.

## ğŸ™ Acknowledgments

- arXiv.org for providing research paper dataset
- CatBoost team for the excellent ML framework
- Flask community for comprehensive documentation
- All contributors and team members

## ğŸ› Known Issues

- Email verification requires valid Gmail SMTP credentials
- Large abstracts may take longer to process
- Mobile UI can be improved for admin dashboard

## ğŸ”® Future Enhancements

- [ ] Support for more academic categories
- [ ] PDF upload and automatic text extraction
- [ ] Batch classification for multiple papers
- [ ] API endpoint for programmatic access
- [ ] Enhanced analytics and visualization
- [ ] Export classification history to CSV
- [ ] Multi-language support
- [ ] Integration with arXiv API
- [ ] Real-time model retraining with user feedback
- [ ] A/B testing for model improvements
- [ ] Docker containerization for easy deployment
